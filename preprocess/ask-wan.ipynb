{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv('~/.secrets')\n",
    "\n",
    "# use open ai to create embeddings\n",
    "openai.api_key = os.getenv('OPENAI_KEY')\n",
    "\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up some data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to the WAN show everyone.',\n",
       " 'The truth is, I screwed up this week.',\n",
       " \"It's very obvious that, I mean, I just couldn't conceal it anymore.\",\n",
       " 'We were paid by NVIDIA for our RTX 4060 TI review.',\n",
       " 'And the scariest part of what I just said is that a not insignificant number of people']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_transcripts(file_name: str):\n",
    "    # load text file transcript.txt \n",
    "    with open(file_name, 'r') as file:\n",
    "        transcript = file.read()\n",
    "    return transcript\n",
    "\n",
    "def clean_transcript(transcript: str):\n",
    "    # remove timestamps from lines\n",
    "    lines = transcript.strip('\\n').split('\\n')\n",
    "    formatted_lines = [line.split(']')[1].strip() for line in lines]\n",
    "    return formatted_lines\n",
    "\n",
    "transcript = load_transcripts('transcript.txt')\n",
    "formatted_lines = clean_transcript(transcript)\n",
    "formatted_lines[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process\n",
    "\n",
    "1. For a given WAN show, create the whisper transcript for it.\n",
    "2. Create embeddings for every line.\n",
    "3. Store the embeddings in Pinecone\n",
    "    - Need to store embeddings\n",
    "    - + also information about the podcast\n",
    "    - For ex timestamp, date / show of the podcast etc.\n",
    "4. (Optional) - Also embed summaries of the WAN show \n",
    "    - For example, Ask an LLM \"What was the topics discussed on today's show\"? and embed it.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and Storing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(line: str):\n",
    "    return openai.Embedding.create(input=[line], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "# create the embedding for store the emebdding and line\n",
    "embedding_type = np.dtype([\n",
    "    ('embedding', np.float64, (1536,)), \n",
    "    ('line', str, 10000),\n",
    "    ('start_time', np.float64),\n",
    "])\n",
    "\n",
    "# initialize the embeddings array\n",
    "embeddings = np.empty(len(formatted_lines), dtype=embedding_type)\n",
    "\n",
    "# load embeddings from file if it exists\n",
    "if os.path.exists('embeddings.npy'):\n",
    "    embeddings = np.load('embeddings.npy', allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the embeddings for a few lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = get_embeddings(\"What are they discussing in the show?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. This makes me feel like -\n",
    "- we can/should embed more than line by line? Because otherwise the vector search is on a very small context.\n",
    "- OR, once the similar line is received, get also +- 5 secs of audio around the line for even more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.04"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_start_end_time(line: str):\n",
    "    # Extract the start and end time string from the square brackets\n",
    "    time_string = line[line.find('[') + 1:line.find(']')]\n",
    "\n",
    "    # Split the time string into start and end time\n",
    "    start_time, end_time = time_string.split(' --> ')\n",
    "\n",
    "    # Parse the start and end time into datetime objects\n",
    "    start_datetime = datetime.strptime(start_time, \"%H:%M:%S.%f\")\n",
    "    end_datetime = datetime.strptime(end_time, \"%H:%M:%S.%f\")\n",
    "\n",
    "    # Extract the time components (hours, minutes, seconds) from datetime objects\n",
    "    start_time = start_datetime.time()\n",
    "    end_time = end_datetime.time()\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "def get_time_diff(start, end):\n",
    "    return (datetime.combine(datetime.min, end) - datetime.combine(datetime.min, start)).total_seconds()\n",
    "\n",
    "# get time in seconds from end\n",
    "def get_time_in_seconds(end):\n",
    "    return (datetime.combine(datetime.min, end) - datetime.min).total_seconds()\n",
    "\n",
    "# parse out timestamps\n",
    "lines = transcript.strip('\\n').split('\\n')\n",
    "\n",
    "# join timestamps for every 20s\n",
    "start, end = extract_start_end_time(lines[1])\n",
    "# get difference in seconds\n",
    "\n",
    "get_time_diff(start, end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings of a certain duration of content..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7a70e7e5a3475cb1450956bcd2884b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the embeddings array\n",
    "embeddings = np.empty(len(formatted_lines), dtype=embedding_type)\n",
    "\n",
    "# variable\n",
    "DURATION = 20\n",
    "\n",
    "embed_text = ''\n",
    "text_duration = 0\n",
    "embed_idx = 0\n",
    "start_time = 0\n",
    "# add tqdm to show progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    raw_line = lines[i]\n",
    "    # get the line start and end time\n",
    "    start, end = extract_start_end_time(raw_line)\n",
    "    # get the difference in seconds\n",
    "    time_diff = get_time_diff(start, end)\n",
    "    # get the raw line\n",
    "    line = raw_line.split(']')[1].strip()\n",
    "\n",
    "    # if the time difference is less than the embed duration, add the line to the embed text\n",
    "    if text_duration < DURATION:\n",
    "        embed_text += line + ' '\n",
    "        text_duration += time_diff\n",
    "        continue\n",
    "\n",
    "    embeddings[embed_idx]['embedding'] = get_embeddings(embed_text)\n",
    "    embeddings[embed_idx]['line'] = embed_text\n",
    "    embeddings[embed_idx]['start_time'] = start_time\n",
    "    # restart from this line\n",
    "    embed_text = line\n",
    "    start_time = get_time_in_seconds(end)\n",
    "    text_duration = 0\n",
    "    embed_idx += 1\n",
    "\n",
    "# remove embeddings with sum 0\n",
    "embeddings = embeddings[~np.all(embeddings['embedding'] == 0, axis=1)]\n",
    "# save embeddings\n",
    "# np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save embeddings whenever necessary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "# np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_context(question: str, most_similar_indices: list):\n",
    "    # append all lines from most similar indices\n",
    "    context = '\\n'.join([embeddings[index]['line'] for index in most_similar_indices])\n",
    "    # create the prompt\n",
    "    prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context:\n",
    "        {context}\n",
    "    \"\"\"\n",
    "\n",
    "    # create the completion\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a bot that answers questions about the WAN show from Linus Tech Tips.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Give Answer in bullet points. Explain in detail.\"},\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def get_context_to_question(question: str):\n",
    "    question_embedding = get_embeddings(question)\n",
    "\n",
    "    # calculate the cosine similarity between the question and each line\n",
    "    similarities = np.array([\n",
    "        cosine_similarity(\n",
    "            np.array(question_embedding).reshape(1, -1), \n",
    "            np.array(embedding['embedding']).reshape(1, -1)\n",
    "        ) \n",
    "        for embedding in embeddings\n",
    "    ])\n",
    "    # get the 5 most similar lines\n",
    "    most_similar_indices = np.argsort(similarities, axis=0)[-10:].flatten()\n",
    "    return most_similar_indices\n",
    "\n",
    "\n",
    "def answer_question(question: str):\n",
    "    # get the most similar indices\n",
    "    most_similar_indices = get_context_to_question(question)\n",
    "    # answer the question\n",
    "    answer = answer_question_from_context(question, most_similar_indices)\n",
    "    return answer, most_similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Topic: Topics Discussed in the WAN Show\n",
       "\n",
       "Bullet points:\n",
       "- Eight Sleep and viewer concerns about mandatory subscriptions\n",
       "- Signalwire sponsorship\n",
       "- Traveling with LAN gear tips\n",
       "- Channel strategy and content evolution\n",
       "- National Eating Disorders Association chatbot controversy\n",
       "- Interesting places merch has been spotted\n",
       "- NVIDIA paid review disclosure\n",
       "- Mega channels and content categorization\n",
       "\n",
       "Explanation:\n",
       "\n",
       "The WAN Show is a weekly show hosted by Linus and Luke from Linus Tech Tips that covers a broad range of technology-related topics. In this particular episode, some of the topics discussed were:\n",
       "\n",
       "- Eight Sleep and viewer concerns about mandatory subscriptions: Eight Sleep, a sponsor of Linus Tech Tips, recently faced criticism from viewers for requiring a mandatory subscription for their products. Linus and Luke discussed the issue and acknowledged viewers' concerns, as well as their own disappointment with the messaging of the product.\n",
       "- Signalwire sponsorship: The show also featured a sponsorship segment for Signalwire, a cloud communications company.\n",
       "- Traveling with LAN gear tips: One viewer asked for tips on traveling with LAN gear, and Linus and Luke jokingly suggested removing the GPU and talked about some of their own experiences with transporting equipment.\n",
       "- Channel strategy and content evolution: The hosts discussed their content strategy and how it has evolved over the years, with a focus on category-specific channels and mega channels like MKBHD and LTT.\n",
       "- National Eating Disorders Association chatbot controversy: The show also briefly touched on a controversy involving the National Eating Disorders Association replacing a helpline with a chatbot.\n",
       "- Interesting places merch has been spotted: Another viewer asked about the most interesting place Linus Tech Tips merch has been spotted, and the hosts shared some fun examples.\n",
       "- NVIDIA paid review disclosure: Linus briefly disclosed that they were paid by NVIDIA for an RTX 4060 TI review, and acknowledged that some viewers might be concerned about the disclosure.\n",
       "- Mega channels and content categorization: Finally, the hosts discussed their thoughts on mega channels and category-specific channels, and how content categorization may play a role in the future of online content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, _ = answer_question(\"What are the topics discussed in the WAN show?\")\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Topic: RTX 4060ti discourse\n",
       "\n",
       "• People are disappointed with the RTX 4060ti review and think that it should have been more critical of NVIDIA.\n",
       "\n",
       "• Some comments suggest that Linus may be supporting NVIDIA because he didn't give a harsh enough critique.\n",
       "\n",
       "• Linus acknowledges that the naming of the RTX 4060ti is a problem, as it is marketed as a 60Ti card but has 50 series performance and is priced higher than it should be.\n",
       "\n",
       "• Linus notes that AMD's product stack is also confusing, with a mix of 60 and 50 series cards.\n",
       "\n",
       "• Linus mentions that they made an error in the review where they incorrectly stated the RTX 4060ti had a 16x interface instead of an 8x interface.\n",
       "\n",
       "• Linus talks about AMD's recent price drops on the 7600XT, which is now priced at $269.\n",
       "\n",
       "• Linus jokes that they were paid by NVIDIA for the review, which is not true.\n",
       "\n",
       "• The truth is that Linus is proud of their RTX 4060ti and RX7600 reviews, but acknowledges that they made some errors.\n",
       "\n",
       "• Reports suggest that the RTX 4060ti has not generated much consumer interest and some retailers are already discounting it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer, _ = answer_question(\"What is the RTX 4060ti discourse about?\")\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    " - Given the answer, get the context it referred to, and then the timestamps, so that you can link it back to the youtube video!\n",
    " - Embed summaries of popular questions (What is the wan show about?)\n",
    " - Specify metadata structure\n",
    "    - line\n",
    "    - date / timestamp\n",
    "    - youtube video link\n",
    "- Play with whisper settings\n",
    "    - Max character limit per generation?\n",
    "    - How many words to translate at a time?\n",
    "- Embed conversations with a little bit of overlap. Maybe use langchain.\n",
    "- Use \"Think about it step by step\" prompting / reAct."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMO, one of the major benefits of video/audio search like this is being able to not only summarize, but also refer to the the snippets of the video / audio for further references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, context = answer_question(\"What is the RTX 4060ti controversy about?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the relevant lines which contributed to the summary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3234.4\n",
      "Let's go. Like, Oh, come on. Oh, well. Anyway, AMD, of course, dropped prices on the 7600 XT down to 269, only 36 hours before launch. Okay, I'd actually like to talk about that. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=3234\n",
      "221.96\n",
      "But we've got a couple of comments in a row here. Like, why can't you just say it's a bad deal? You're not crapping on Nvidia makes it look like you're caping for Nvidia. Not saying you are, but that's what it looks like. This review also feels short and unrigorous. This is a rare thumbs down for me. We had probably more game benchmarking than just them we've ever had before. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=221\n",
      "410.28\n",
      "It doesn't affect the numbers that we showed you guys, but that is something that we could have gotten a little bit better. I think there were a couple small things in the 7600. But every time we release a new GPU review, we're getting a little bit cleaner about it. It's a lot of moving parts, guys. But this is one of those things that I'm just looking at going, \"Give me a reason not to \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=410\n",
      "831.8\n",
      "All right. I set a budget for myself of approximately the price of a 40... 4060 Ti. Okay. I managed to beat it by about 30% while still saving like 20 bucks. Oh, performance beat it by 30 bucks. Oh yeah. And save 20 bucks. Oh yeah. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=831\n",
      "699.88\n",
      "I like dislike ratio on that video. It's still about 92%, 93% or something like that. It might even be higher now. Because it tends to be that it's that initial rush of people, like the really hardcore tech people that are like, \"Oh, this review wasn't critical enough,\" or whatever the case may be. But the number of people that are mad at me for not going hard enough at NVIDIA is if I have any kind of influence whatsoever on NVIDIA. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=699\n",
      "634.6\n",
      "That's its problem. Yeah. NVIDIA is marketing a 50 series card as a 60 Ti card. And that's a problem. And pricing it as such. And that's a problem. But it doesn't make it garbage. It makes it overpriced. Like if you found it and it was that card and it was on a mad discount. Yeah. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=634\n",
      "3729.48\n",
      "the buck GPU for around the price of a 4060 Ti at retail, I ended up doing a bunch of research into AMD cards that I hadn't really paid much attention to. Their product stack is a mess. They have a 6600, a 6600 XT and then they have a 6650 XT. But then in the 7000 generation, they're not doing 50s. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=3729\n",
      "0.0\n",
      "Welcome to the WAN show everyone. The truth is, I screwed up this week. It's very obvious that, I mean, I just couldn't conceal it anymore. We were paid by NVIDIA for our RTX 4060 TI review. And the scariest part of what I just said is that a not insignificant number of people \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=0\n",
      "385.44\n",
      "I just want to give a huge shout out to the writers, huge shout out to the lab. I'm actually really, really proud of our 4060 Ti review and our RX7600 review. There are a couple of things we got wrong. I think we had a table in the 4060 Ti review that said it had a 16x interface. It has an 8x interface. That is a problem if you're upgrading an older platform. \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=385\n",
      "3155.68\n",
      "deal from NVIDIA to promote their new GPU slash S. NVIDIA's new RTX 4060 TI has been met with low consumer interest. Many retailers only ordered a few cards per store. And in some areas, especially Europe, they're already offering the cards for $10 to $25 \n",
      "https://www.youtube.com/watch?v=0vP5Knq1xhs&t=3155\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=0vP5Knq1xhs\"\n",
    "\n",
    "def get_video_timestamp(main_url: str, start_time: float):\n",
    "    return f\"{main_url}&t={int(start_time)}\"\n",
    "\n",
    "for index in context[-10:]:\n",
    "    print(embeddings[index]['start_time'])\n",
    "    print(embeddings[index]['line'])\n",
    "    print(get_video_timestamp(video_url, embeddings[index]['start_time']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, we need to have the timestamps as part of the embedding too, otherwise it will be hard to know where it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/watch?v=0vP5Knq1xhs?t=0'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_timestamp(video_url, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting embeddings into postgres with pgvector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Integer, String, create_engine, text, JSON, insert, Float\n",
    "from sqlalchemy.orm import declarative_base, mapped_column, Session\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "postgres_pwd = os.environ.get('POSTGRES_PWD')\n",
    "# supabase engine\n",
    "db_uri = 'postgresql://postgres:{}@db.wiagwfwjtbojsbzmeduv.supabase.co:5432/postgres'.format(postgres_pwd)\n",
    "engine = create_engine(db_uri)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Document(Base):\n",
    "    \"\"\"\n",
    "    A class used to represent a Document\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = 'docs'\n",
    "\n",
    "    id = mapped_column(Integer, primary_key=True, autoincrement=True)\n",
    "    embedding = mapped_column(Vector(1536))\n",
    "    line = mapped_column(String)\n",
    "    meta = mapped_column(JSON)\n",
    "    video_url = mapped_column(String)\n",
    "    timestamp = mapped_column(Float)\n",
    "    created_at = mapped_column(String, server_default=text('NOW()'))\n",
    "    \n",
    "\n",
    "# TODO: what does this line do?\n",
    "Base.metadata.drop_all(engine)\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "video_url='https://www.youtube.com/watch?v=0vP5Knq1xhs'\n",
    "documents = [\n",
    "    dict(\n",
    "        embedding=embedding['embedding'], \n",
    "        line=embedding['line'],\n",
    "        meta={\n",
    "            'line': embedding['line'], \n",
    "            'start_time': embedding['start_time']\n",
    "        },\n",
    "        video_url=video_url,\n",
    "        timestamp=embedding['start_time']\n",
    "    ) for embedding in embeddings\n",
    "]\n",
    "\n",
    "session = Session(engine)\n",
    "session.execute(insert(Document), documents)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
